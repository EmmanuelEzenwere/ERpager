{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import sys\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "import pickle \n",
    "import joblib\n",
    "import dill\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sqlalchemy import create_engine\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "#help(sklearn.multioutput)\n",
    "# help(sklearn)\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion,Pipeline\n",
    "\n",
    "\n",
    "#help(sklearn.metrics)\n",
    "#help(sklearn.multioutput)\n",
    "\n",
    "# Download the stopwords and all nltk relevant packages.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    database_path=\"../data/DisasterResponse.db\",\n",
    "    x_column_name = \"message\", \n",
    "    y_start=4, \n",
    "    y_stop=None, \n",
    "    table_name='cleandata'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Fetch cleaned data from an sqlite database and return as \n",
    "    pandas data frame.\n",
    "\n",
    "    Args:\n",
    "       x_column_name (str): Name of the column in the dataset table\n",
    "                             for input features (default is 'message').\n",
    "        y_start (int): The starting index for the target columns (default is 4).\n",
    "        y_stop (int or None): The stopping index for the target columns.\n",
    "                              If None, all columns from y_start onward will be \n",
    "                              selected (default is None).\n",
    "        table_name (str): Name of the table in the database containing the dataset\n",
    "                          (default is 'cleandata').\n",
    "        database_path (str): Relative path to the database file containing the \n",
    "                             cleaned dataset (default is 'DisasterTweets.db').\n",
    "    \n",
    "    Returns:\n",
    "        X (numpy.ndarray): Input feature data.\n",
    "        y (numpy.ndarray): Target output data.\n",
    "        df (pandas.DataFrame): DataFrame containing both input features and target outputs.\n",
    "    \"\"\"\n",
    "    conn_engine = create_engine(\"sqlite:///\"+database_path)\n",
    "    # Load data from a specific table into a DataFrame\n",
    "    df = pd.read_sql_table(table_name, con=conn_engine)\n",
    "    # Extract the input feature column\n",
    "    X = df[x_column_name]\n",
    "    # Extract the target columns\n",
    "    if y_stop == None:\n",
    "        y = df.iloc[:,y_start:]\n",
    "    else:\n",
    "        y = df.iloc[:, y_start:y_stop] \n",
    "    classes = list(y.columns)\n",
    "    return X.values, y.values, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_data(x_train, y_train, class_names, x_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Performs a comprehensive audit of machine learning dataset properties and types.\n",
    "    \n",
    "    Args:\n",
    "        x_train(numpy.ndarray): Training features\n",
    "        y_train(numpy.ndarray): Training labels\n",
    "        class_names(list): list of all class names in the output dataset.\n",
    "        x_test(numpy.ndarray): Test features -- optional\n",
    "        y_test(numpy.ndarray): Test labels -- optional\n",
    "        \n",
    "    Returns:\n",
    "        data_info(pd.DataFrame) : Pandas data frame containing data category, \n",
    "                                shape and type info.\n",
    "    \"\"\"\n",
    "    print(\"-\"*100)\n",
    "    print(\"Dataset Audit Report\")\n",
    "    \n",
    "    # View the first few rows of the DataFrame\n",
    "    x_df = pd.DataFrame(x_train)\n",
    "    y_df = pd.DataFrame(y_train)\n",
    "    x_df.columns =[\"input_features\"]\n",
    "    y_df.columns = list(class_names)\n",
    "    display(x_df.head(3))\n",
    "    display(y_df.head(3))\n",
    "\n",
    "    data_info = {\n",
    "        \"data\":[],\n",
    "        \"type\":[],\n",
    "        \"shape\":[]\n",
    "    }\n",
    "    \n",
    "    data_info['data'].append('X_train')\n",
    "    data_info['type'].append(type(x_train))\n",
    "    data_info['shape'].append(x_train.shape)\n",
    "    data_info['data'].append('y_train')\n",
    "    data_info['type'].append(type(y_train))\n",
    "    data_info['shape'].append(y_train.shape)\n",
    "    \n",
    "    \n",
    "    if x_test is not None and y_test is not None:\n",
    "        data_info['data'].append('X_test')\n",
    "        data_info['type'].append(type(x_test))\n",
    "        data_info['shape'].append(x_test.shape)\n",
    "        data_info['data'].append('y_test')\n",
    "        data_info['type'].append(type(y_test))\n",
    "        data_info['shape'].append(y_test.shape)\n",
    "    \n",
    "    data_info = pd.DataFrame(data_info)\n",
    "    display(data_info)\n",
    "\n",
    "    # Dataframe column of class names in the dataset.\n",
    "    class_df = pd.DataFrame({\"labels\": class_names})\n",
    "    \n",
    "    # Count occurrences of each class label in the training data.\n",
    "    class_freq = []\n",
    "    for index in range(len(class_names)):\n",
    "        class_count = np.sum(y_train[:,index])\n",
    "        class_freq.append(class_count)\n",
    "    \n",
    "    # Create a DataFrame with counts for each class in the training data\n",
    "    # Add the category/class names as a column\n",
    "\n",
    "    train_data_distr = pd.DataFrame({\n",
    "        \"labels\": class_names,\n",
    "        \"no. of samples(Train Data)\": class_freq    \n",
    "    })\n",
    "    \n",
    "    data_distr_df = train_data_distr\n",
    "    \n",
    "    if x_test is not None and y_test is not None:\n",
    "        # Count occurrences of each class label in the test data.\n",
    "        class_freq = []\n",
    "        for index in range(len(class_names)):\n",
    "            class_count = np.sum(y_test[:,index])\n",
    "            class_freq.append(class_count)\n",
    "        \n",
    "        # Create a DataFrame with counts for each class in the test data\n",
    "        # Add the category/class names as a column\n",
    "\n",
    "        test_data_distr = pd.DataFrame({\n",
    "            \"labels\": class_names,\n",
    "            \"no. of samples(Test Data)\": class_freq    \n",
    "        })\n",
    "        \n",
    "        data_distr_df = pd.concat([data_distr_df, test_data_distr.iloc[:,1:]], axis=1)\n",
    "    \n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # Running in a Jupyter Notebook.\n",
    "        print(\"-\"*100)\n",
    "        print(\"Data Distribution in Dataset:\")\n",
    "        display(data_distr_df)\n",
    "\n",
    "    else:\n",
    "        # Not running in a Jupyter Notebook.\n",
    "        print(\"-\"*100)\n",
    "        print(\"Data Distribution in Dataset:\")\n",
    "        print(data_distr_df)  \n",
    "    \n",
    "    print(\"\\n\")\n",
    "            \n",
    "    return data_info        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_multiclass_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for multi-class classification\n",
    "    \n",
    "    Args:\n",
    "        y_test(numpy.ndarray): actual output data.\n",
    "        y_pred(numpy.ndarray): predicted output data as returned by our model,\n",
    "                              classifier.\n",
    "    \n",
    "    Returns:\n",
    "        summary_accuracy_score(float): summary accuracy using decoded labels.\n",
    "    \"\"\"\n",
    "    # Convert to labels if one-hot encoded\n",
    "    if (len(y_true.shape) > 1) & (len(y_pred.shape) > 1):\n",
    "        # Initialize an empty list to store per-column accuracy\n",
    "        accuracies = []\n",
    "\n",
    "        # Iterate over each column (class output)\n",
    "        for col in range(y_true.shape[1]):\n",
    "            # Compute accuracy for the current column\n",
    "            col_accuracy = accuracy_score(y_true[:, col], y_pred[:, col])\n",
    "            accuracies.append(col_accuracy)\n",
    "\n",
    "        # Average accuracy across all columns\n",
    "        summary_accuracy_score = np.mean(accuracies)\n",
    "       \n",
    "    else:\n",
    "        print(\"Empty values for y test or y pred\")\n",
    "        summary_accuracy_score = np.nan\n",
    "    \n",
    "    return summary_accuracy_score\n",
    "\n",
    "\n",
    "def per_class_accuracy(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for each class separately\n",
    "\n",
    "    Args\n",
    "        y_test(numpy.ndarray): actual output data.\n",
    "        y_pred(numpy.ndarray): predicted output data as returned by our model,\n",
    "                              classifier.\n",
    "        class_names (list): list of all class names in the output dataset.\n",
    "    \n",
    "    return\n",
    "        accuracies_df (pd.DataFrame): a dataframe of class names \n",
    "        and their correspoinding accuracy score for binary classification\n",
    "    \n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    for i in range(len(class_names)):\n",
    "        class_data_true = y_true[:, i]\n",
    "        class_data_pred = y_pred[:, i]\n",
    "        if (class_data_true.shape[0] > 0) & (class_data_pred.shape[0] > 0):\n",
    "            class_acc = accuracy_score(class_data_true, class_data_pred)\n",
    "            accuracies.append(class_acc)\n",
    "            \n",
    "        else:\n",
    "            print(\"Empty values for y test or y pred\")\n",
    "            class_acc = np.nan\n",
    "            accuracies.append(class_acc)\n",
    "            \n",
    "    \n",
    "    accuracies_df = pd.DataFrame({\"category\":class_names, \"accuracy\":accuracies})\n",
    "    return accuracies_df\n",
    "    \n",
    "\n",
    "def evaluate_multilabel_model(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a multi-label classification model.\n",
    "    \n",
    "    Args:\n",
    "        y_test (numpy.ndarray): actual output data.\n",
    "        y_pred (numpy.ndarray): predicted output data as returned by our model,\n",
    "                              classifier.\n",
    "        class_names (list): list of all class names in the output dataset.\n",
    "        \n",
    "    return\n",
    "        metrics_df(pd.DataFrame): a dataframe of class names \n",
    "                              and their correspoinding precision, recall and \n",
    "                              F1-score values, for binary classification\n",
    "    \"\"\"\n",
    "    average_accuracy = calculate_multiclass_accuracy(y_true, y_pred)\n",
    "    print(f\"Overall Sample-wise Accuracy: {average_accuracy:.3f}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    # Calculate per-class accuracy\n",
    "    accuracies_df = per_class_accuracy(y_true, y_pred, class_names)\n",
    "\n",
    "    # Per-class classification metrics\n",
    "    classification_metrics = []\n",
    "    reports = {}\n",
    "    for i in range(y_true.shape[1]):\n",
    "        if np.sum(y_true[:, i]) > 0:  # Check if there are true instances\n",
    "            precision = precision_score(y_true[:, i], y_pred[:, i], average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true[:, i], y_pred[:, i],average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true[:, i], y_pred[:, i],average='macro', zero_division=0)\n",
    "        \n",
    "            classification_metrics.append({\n",
    "                'Class': class_names[i],\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1-Score': f1\n",
    "            })\n",
    "        \n",
    "            \n",
    "            report_dict = classification_report(y_true[:, i], y_pred[:, i], zero_division=0, output_dict=True)\n",
    "            report_df = pd.DataFrame(report_dict).transpose()\n",
    "            report_df = report_df.round(3)\n",
    "            report_log = None\n",
    "            reports[class_names[i]]=[report_df, report_log]\n",
    "            \n",
    "        else:\n",
    "            classification_metrics.append({\n",
    "                'Class': class_names[i],\n",
    "                'Precision': np.nan,\n",
    "                'Recall': np.nan,\n",
    "                'F1-Score': np.nan\n",
    "            })\n",
    "            report_df = None\n",
    "            reports_log = f\"\\nFound No true instances for {class_names[i]}. Ommitted from report\\n\"\n",
    "            reports[class_names[i]]=[report_df, reports_log]\n",
    "            \n",
    "    # Create a DataFrame with a summary classification metrics per class.\n",
    "    classification_metrics_df = pd.DataFrame(classification_metrics)\n",
    "    print(\"\\nSummary of all metrics:\")\n",
    "    print(\"-\"*100)\n",
    "    metrics_df = pd.concat([accuracies_df, classification_metrics_df.iloc[:,1:]], axis=1).round(3)\n",
    "    \n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # Running in a Jupyter Notebook.\n",
    "        display(metrics_df.round(3))\n",
    "\n",
    "    else:\n",
    "        # Not running in a Jupyter Notebook.\n",
    "        print(metrics_df.round(3))\n",
    "        \n",
    "    \n",
    "    print('Detailed classification report per class')\n",
    "    print(\"-\"*100)\n",
    "    for class_name in reports.keys():\n",
    "        print(f\"\\nDetailed metrics for {class_name}:\")\n",
    "        report_df = reports[class_name][0]\n",
    "        if report_df is not None:\n",
    "            if 'ipykernel' in sys.modules:\n",
    "                # Running in a Jupyter Notebook.\n",
    "                display(report_df)\n",
    "\n",
    "            else:\n",
    "                # Not running in a Jupyter Notebook.\n",
    "                print(report_df)\n",
    "        else:\n",
    "            report_log = reports[class_name][1]\n",
    "            print(report_log)\n",
    "        \n",
    "    # Visualize metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics_melted = pd.melt(classification_metrics_df, id_vars=['Class'], \n",
    "                           value_vars=['Precision', 'Recall', 'F1-Score'])\n",
    "    sns.barplot(x='Class', y='value', hue='variable', data=metrics_melted)\n",
    "    plt.xticks(rotation=80)\n",
    "    plt.title('Model Performance Metrics by Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "        \n",
    "        \n",
    "def model_evaluator(model, x_test, y_test, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate the model and print all relevant metrics\n",
    "    \n",
    "    Args:\n",
    "        x_test(numpy.ndarray): actual input data\n",
    "        y_test(numpy.ndarray): actual output data.\n",
    "        class_names (list): list of all class names in the output dataset.\n",
    "    \n",
    "    Returns:\n",
    "        y_pred(numpy.ndarray): predicted output data as returned by our model,\n",
    "                              classifier.\n",
    "        metrics_df(pd.DataFrame): a dataframe of class names \n",
    "                              and their correspoinding precision, recall and \n",
    "                              F1-score values, for binary classification\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"-\"*100)\n",
    "    print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Get best parameters if using GridSearchCV\n",
    "    if hasattr(model, 'best_params_'):\n",
    "        print('ML-Pipeline Model')\n",
    "        print(\"-\"*100)\n",
    "        print(\"\\nBest cross-validation score:\", model.best_score_)\n",
    "        print(\"\\nBest parameters found:\")\n",
    "        pprint(model.best_params_)\n",
    "        print(\"-\"*100)\n",
    "    else:\n",
    "        print('Non-ML Pipeline Model')\n",
    "        print(\"-\"*100)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics_df = evaluate_multilabel_model(y_test, y_pred, class_names)\n",
    "  \n",
    "    return y_pred, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, stop_words=None):\n",
    "    \"\"\"\n",
    "    Tokenize a text by normalizing, lemmatizing and removing stop words.\n",
    "    \n",
    "    Args:\n",
    "        text (list): list of strings\n",
    "        stop_words (set): a set of word strings for stop words -- optional.\n",
    "\n",
    "    Returns:\n",
    "        tokens(list): list of token strings.\n",
    "    \"\"\"\n",
    "    # Import stopwords if not imported.\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    \n",
    "    url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Replace URLs with a placeholder and normalize case.\n",
    "    normalized_text = re.sub(url_regex, ' ', text.lower())\n",
    "\n",
    "    # Replace non-alphanumeric characters with spaces.\n",
    "    normalized_text = re.sub(r'[^a-zA-Z0-9]', ' ', normalized_text)\n",
    "    \n",
    "    tokens = word_tokenize(normalized_text)\n",
    "        \n",
    "    # Lemmatize with POS tagging\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            pos_tag = nltk.pos_tag([token])[0][1]\n",
    "            \n",
    "            # Special handling for adverbs ending in 'ly'\n",
    "            if pos_tag.startswith('RB') and token.endswith('ly'):\n",
    "                base_form = token[:-2]  # Remove 'ly'\n",
    "                clean_token = lemmatizer.lemmatize(base_form, pos='a')\n",
    "            elif pos_tag.startswith('VB'):\n",
    "                clean_token = lemmatizer.lemmatize(token, pos='v')\n",
    "            elif pos_tag.startswith('JJ'):\n",
    "                clean_token = lemmatizer.lemmatize(token, pos='a')\n",
    "            else:\n",
    "                clean_token = lemmatizer.lemmatize(token, pos='n')\n",
    "            \n",
    "            clean_tokens.append(clean_token)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Generates a new binary input feature, (1 if the first word in the input text is a verb, 0 otherwise),\n",
    "        This feature will be combined with the input text data processed through a TF-IDF transformer pipeline.\n",
    "        \"\"\"\n",
    "        self.run_count = 1\n",
    "        print(\"Start Verb Extractor running...\")\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        \"\"\"\n",
    "        Tags the first word of the tokenized input text.\n",
    "        \n",
    "        Args:\n",
    "            text(string): a row input text message.\n",
    "        returns:\n",
    "             tag(integer): 1 if the first word is a verb, 0 otherwise.\n",
    "        \"\"\"\n",
    "        self.run_count += 1\n",
    "        try:\n",
    "            # Check for RT (retweet) first\n",
    "            if text.strip().upper().startswith('RT'):\n",
    "                return 1\n",
    "        \n",
    "            # print(\"\\n\\nText:\", text)\n",
    "            first_sentence = nltk.sent_tokenize(text)[0]\n",
    "            \n",
    "            # Tokenize the first_sentence\n",
    "            text_tokens = tokenize(first_sentence) \n",
    "            \n",
    "            if not text_tokens:\n",
    "                # No text token in first sentence or first sentence in text is empty\n",
    "        \n",
    "                # print(f'Empty Text Tokens, {text_tokens} in first \"{first_sentence}\"')\n",
    "                # print(f\"Non-Text first word({self.run_count}): \")\n",
    "                tag = 0\n",
    "                return tag\n",
    "            \n",
    "            # Get the POS (parts of speech) of the words in the text.\n",
    "            first_word, first_tag = nltk.pos_tag(text_tokens)[0]\n",
    "            \n",
    "            # Check if the first word is a Verb. \n",
    "            if first_tag in ['VB', 'VBP', 'UH']:\n",
    "                # print(\"\\nVerb, Tag: \",first_tag, \", First Word: \",first_word)\n",
    "                tag = 1\n",
    "                return tag\n",
    "            else:\n",
    "                # print(f\"\\nNon-verb, Tag: {first_tag}, First Word: ,{first_word}\")\n",
    "                tag = 0\n",
    "                return tag\n",
    "                \n",
    "        except Exception as e:\n",
    "            # print(f\"Unexpected error: {e}\")\n",
    "            # print(f\"Run No.({self.run_count})\")\n",
    "            # print(f\"Text causing issue: {text}\")\n",
    "            tag = 0\n",
    "            return tag\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform method.\n",
    "        \n",
    "        Args:\n",
    "            X(numpy.ndarray):input column vector of text messages. Each row is a \n",
    "                             sample text message.\n",
    "        returns:\n",
    "            df_array(numpy.ndarray):a new column binary vector. Each row is a \n",
    "                             1 or 0 (1 if the first word in the corresponding \n",
    "                             row in the input column vector is a verb, and 0,\n",
    "                             otherwise).\n",
    "        \"\"\"\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        df_array = pd.DataFrame(X_tagged).values # converts to a 2D numpy array.\n",
    "        # df = X_tagged.values # removing this because the hstack fails.\n",
    "        \n",
    "        # Log information about the transformation\n",
    "        print(\"\\n\\nFeature Extraction and Text Transformation Complete:\")\n",
    "        print(\"Extracted/New feature shape:\", df_array.shape)\n",
    "        if type(X) == list:\n",
    "            print(\"Input feature shape: \", len(X))\n",
    "        else:\n",
    "            print(\"Input feature shape: \", X.shape)\n",
    "        \n",
    "        return df_array\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_pipeline(model, X_train):\n",
    "    \"\"\"\n",
    "    Safely inspect each step of a fitted pipeline.\n",
    "    \n",
    "    Args:\n",
    "        model(GridSearchCV): A fitted GridSearchCV object containing a pipeline\n",
    "            with preprocessing and model steps. Must be already fitted.\n",
    "        X_train: Training data to inspect transformations\n",
    "    \"\"\"\n",
    "    if not isinstance(model, GridSearchCV):\n",
    "        print(\"Warning: Model is not a GridSearchCV instance\")\n",
    "        return\n",
    "    \n",
    "    print(\"Best Parameters:\", model.best_params_)\n",
    "    print(\"\\nInspecting Pipeline Steps:\")\n",
    "    \n",
    "    # Access the fitted pipeline\n",
    "    pipeline = model.best_estimator_\n",
    "    current_data = X_train.copy()\n",
    "    \n",
    "    # Iterate through named steps\n",
    "    for name, transformer in pipeline.named_steps.items():\n",
    "        print(f\"\\nStep: {name}\")\n",
    "        print(f\"Input shape: {current_data.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if step has transform method\n",
    "            if hasattr(transformer, 'transform'):\n",
    "                current_data = transformer.transform(current_data)\n",
    "                print(f\"Output shape: {current_data.shape}\")\n",
    "                \n",
    "                # Print feature names if available\n",
    "                if hasattr(transformer, 'get_feature_names_out'):\n",
    "                    features = transformer.get_feature_names_out()\n",
    "                    print(f\"First few features: {features[:5]}\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"Note: {name} doesn't have transform method (might be final estimator)\")\n",
    "                \n",
    "            # Print additional info for specific transformer types\n",
    "            if hasattr(transformer, 'n_features_in_'):\n",
    "                print(f\"Number of input features: {transformer.n_features_in_}\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {str(e)}\\n\")\n",
    "            continue\n",
    "    \n",
    "    return current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filepath, overwrite=True):\n",
    "    \"\"\"\n",
    "    Saves the fitted ML model as a pickle file.\n",
    "\n",
    "    Args:\n",
    "        model (pipeline or GridSearchCV object): The fitted ML model to be saved.\n",
    "        filepath (str): Specified filename/ filepath to save the ML model.\n",
    "        overwrite (bool): If False, raise error if file exists. Default True.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(\"Current Working Directory:\", os.getcwd())\n",
    "        \n",
    "        if not overwrite and os.path.exists(filepath):\n",
    "        # Check if file exists and overwrite is False\n",
    "            raise FileExistsError(f\"File {filepath} already exists and overwrite=False\")\n",
    "            \n",
    "        # Open file in binary write mode\n",
    "        with open(filepath, 'wb') as f:\n",
    "            joblib.dump(model, f)\n",
    "        print(f\"Model saved successfully as {filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath):\n",
    "    \"\"\"Load ML model using joblib\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model = joblib.load(f)\n",
    "        print(f\"Model loaded successfully from {filepath}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "db_path = '../data/DisasterResponse.db'\n",
    "X, y, classes = load_data(\n",
    "                        x_column_name=\"message\",\n",
    "                        y_start=4,\n",
    "                        table_name=\"cleandata\",\n",
    "                        database_path=db_path\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the training dataset\n",
    "data_audit_report = audit_data(X, y, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer(tokenizer=tokenize)),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "data_audit_report = audit_data(X_train, y_train, classes, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, metrics_df = model_evaluator(model, X_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the outputs of each transformer is of the shape we expect.\n",
    "output_data = inspect_pipeline(model, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_model():\n",
    "    \"\"\"\n",
    "    Constructs an ML pipeline for processing text data using a CountVectorizer and \n",
    "    TF-IDF transformer, followed by a multi-output classifier with Random Forest.\n",
    "\n",
    "    The pipeline is designed for multi-label classification and uses GridSearchCV\n",
    "    to fine-tune hyperparameters, such as n-gram range and Random Forest parameters.\n",
    "\n",
    "    Returns:\n",
    "        model (GridSearchCV): A grid search object for training and evaluation, \n",
    "        which includes feature extraction, transformation, and a classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_pipeline = Pipeline([\n",
    "        (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer())\n",
    "    ])\n",
    "    \n",
    "     \n",
    "    model = Pipeline([\n",
    "        (\"features\", text_pipeline),\n",
    "        (\"clf\", MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "        'features__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'clf__estimator__n_estimators': [50, 100, 200],\n",
    "        'clf__estimator__min_samples_split': [2, 3, 4]\n",
    "    }\n",
    "    \n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(estimator=model, param_grid=parameters, verbose=2, cv=3, error_score='raise')\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audit_report = audit_data(X_train, y_train, classes, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = gridsearch_model()\n",
    "new_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the outputs of each transformer is of the shape we expect.\n",
    "output_data = inspect_pipeline(new_model, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, metrics_df = model_evaluator(new_model, X_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Constructs an ML pipeline using FeatureUnion to add a new binary feature.\n",
    "    The feature checks if the first word in each text is a verb (1 if a verb, 0 otherwise),\n",
    "    and combines this feature with the text data processed through a TF-IDF transformer pipeline.\n",
    "    The combined feature matrix is used to train the model to enhance its performance.\n",
    "\n",
    "    Returns:\n",
    "        model (Pipeline): A scikit-learn pipeline object for training and evaluation, \n",
    "        which includes feature extraction, transformation, and a classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize(text, stop_words=None):\n",
    "        \"\"\"\n",
    "        Tokenize a text by normalizing, lemmatizing and removing stop words.\n",
    "        \n",
    "        Args:\n",
    "            text (list): list of strings\n",
    "            stop_words (set): a set of word strings for stop words -- optional.\n",
    "\n",
    "        Returns:\n",
    "            tokens(list): list of token strings.\n",
    "        \"\"\"\n",
    "        # Import stopwords if not imported.\n",
    "        if stop_words is None:\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()    \n",
    "        \n",
    "        url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        \n",
    "        # Replace URLs with a placeholder and normalize case.\n",
    "        normalized_text = re.sub(url_regex, ' ', text.lower())\n",
    "\n",
    "        # Replace non-alphanumeric characters with spaces.\n",
    "        normalized_text = re.sub(r'[^a-zA-Z0-9]', ' ', normalized_text)\n",
    "        \n",
    "        tokens = word_tokenize(normalized_text)\n",
    "            \n",
    "        # Lemmatize with POS tagging\n",
    "        clean_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                pos_tag = nltk.pos_tag([token])[0][1]\n",
    "                \n",
    "                # Special handling for adverbs ending in 'ly'\n",
    "                if pos_tag.startswith('RB') and token.endswith('ly'):\n",
    "                    base_form = token[:-2]  # Remove 'ly'\n",
    "                    clean_token = lemmatizer.lemmatize(base_form, pos='a')\n",
    "                elif pos_tag.startswith('VB'):\n",
    "                    clean_token = lemmatizer.lemmatize(token, pos='v')\n",
    "                elif pos_tag.startswith('JJ'):\n",
    "                    clean_token = lemmatizer.lemmatize(token, pos='a')\n",
    "                else:\n",
    "                    clean_token = lemmatizer.lemmatize(token, pos='n')\n",
    "                \n",
    "                clean_tokens.append(clean_token)\n",
    "        \n",
    "        return clean_tokens\n",
    "\n",
    "    text_pipeline = Pipeline([\n",
    "        (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer())\n",
    "    ])\n",
    "    \n",
    "    features = FeatureUnion([\n",
    "        (\"text_pipeline\",text_pipeline),\n",
    "        (\"starting_verb\", StartingVerbExtractor())\n",
    "    ])\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"features\", features),\n",
    "        (\"clf\", MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': [(1, 2)],\n",
    "        'clf__estimator__n_estimators': [200],\n",
    "        'clf__estimator__min_samples_split': [2]\n",
    "    }\n",
    "    \n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(estimator=model, param_grid=parameters, verbose=2, cv=3, error_score='raise')\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# class_weight='balanced'\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audit_report = audit_data(X_train, y_train, classes, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = build_model()\n",
    "new_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the outputs of each transformer is of the shape we expect.\n",
    "output_data = inspect_pipeline(new_model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, metrics_df = model_evaluator(new_model, X_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    ML model run script.\n",
    "    \"\"\"\n",
    "    # load data from database\n",
    "    db_path = '../data/DisasterResponse.db'\n",
    "    X, y, classes = load_data(\n",
    "                            x_column_name=\"message\",\n",
    "                            y_start=4,\n",
    "                            table_name=\"cleandata\",\n",
    "                            database_path=db_path\n",
    "                        )\n",
    "                                        \n",
    "    # Inspect the training dataset\n",
    "    data_audit_report = audit_data(X, y, classes)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "    \n",
    "    # Inspect the split training and test dataset   \n",
    "    data_audit_report = audit_data(X_train, y_train, classes, X_test, y_test)\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # ensure the outputs of each transformer is of the shape we expect.\n",
    "    output_data = inspect_pipeline(model, X_train)\n",
    "    \n",
    "    # evaluate the model with test data.\n",
    "    y_pred, metrics_df = model_evaluator(model, X_test, y_test, classes) \n",
    "    \n",
    "    model_path = '../models/classifier.pkl'\n",
    "    save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../data/DisasterResponse.db'\n",
    "X, y, classes = load_data(\n",
    "                        x_column_name=\"message\",\n",
    "                        y_start=4,\n",
    "                        table_name=\"cleandata\",\n",
    "                        database_path=db_path\n",
    "                    )\n",
    "                                    \n",
    "# Inspect the training dataset\n",
    "data_audit_report = audit_data(X, y, classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "# Inspect the split training and test dataset   \n",
    "data_audit_report = audit_data(X_train, y_train, classes, X_test, y_test)\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ensure the outputs of each transformer is of the shape we expect.\n",
    "output_data = inspect_pipeline(model, X_train)\n",
    "\n",
    "# evaluate the model with test data.\n",
    "y_pred, metrics_df = model_evaluator(model, X_test, y_test, classes) \n",
    "\n",
    "model_path = 'classifier.pkl'\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERIMENTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_model():\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    model = Pipeline([\n",
    "        (\"vectorize\", CountVectorizer(tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../data/DisasterResponse.db'\n",
    "X, y, classes = load_data(\n",
    "                        x_column_name=\"message\",\n",
    "                        y_start=4,\n",
    "                        table_name=\"cleandata\",\n",
    "                        database_path=db_path\n",
    "                    )\n",
    "                                    \n",
    "# Inspect the training dataset\n",
    "data_audit_report = audit_data(X, y, classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "# Inspect the split training and test dataset   \n",
    "data_audit_report = audit_data(X_train, y_train, classes, X_test, y_test)\n",
    "\n",
    "model = mock_model()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ensure the outputs of each transformer is of the shape we expect.\n",
    "output_data = inspect_pipeline(model, X_train)\n",
    "\n",
    "# evaluate the model with test data.\n",
    "y_pred, metrics_df = model_evaluator(model, X_test, y_test, classes) \n",
    "\n",
    "model_path = 'mock_classifier.pkl'\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filepath = \"mock-classifier.pkl\"\n",
    "save_model(model, model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_filepath)\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the outputs of each transformer is of the shape we expect.\n",
    "output_data = inspect_pipeline(loaded_model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, metrics_df = model_evaluator(loaded_model, X_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
