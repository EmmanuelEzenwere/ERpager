{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# load data from database\n",
    "engine =  create_engine('sqlite:///DisasterTweets.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(table_name, conn_engine):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       table_name(\"String\"):\n",
    "       conn_engine\"\n",
    "    \"\"\"\n",
    "    # Load data from a specific table into a DataFrame\n",
    "    table_name = 'cleandata'\n",
    "    df = pd.read_sql_table(table_name, con=conn_engine)\n",
    "\n",
    "    X = df[\"message\"]\n",
    "    Y = df.iloc[:,4:]\n",
    "    \n",
    "    return X, Y, df\n",
    "\n",
    "text_inputs, response_labels, df = load_data(table_name=\"cleandata\", conn_engine=engine)\n",
    "X, Y = text_inputs.values, response_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset(X_train, y_train, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"unique Y values: \", np.unique(Y))\n",
    "    print(\"training set, X: \", X_train.shape)\n",
    "    if X_test is not None:\n",
    "        print(\"test set, X: \",X_test.shape)\n",
    "    print(\"training set, Y: \",y_train.shape)\n",
    "    if y_test is not None:\n",
    "        print(\"test set, Y: \",y_test.shape)\n",
    "        \n",
    "def data_type_check(X1, X2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # check data types of \n",
    "    print(\"X1 shape: \", X1.shape)\n",
    "    print(\"X2 shape: \", X2.shape)\n",
    "    print(\"X1 Type: \", type(X1))\n",
    "    print(\"X2 Type: \",type(X2))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique Y values:  [0 1 2]\n",
      "training set, X:  (26386,)\n",
      "training set, Y:  (26386, 36)\n",
      "\n",
      "labels: ['related', 'request', 'offer', 'aid_related', 'medical_help', 'medical_products', 'search_and_rescue', 'security', 'military', 'child_alone', 'water', 'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport', 'buildings', 'electricity', 'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure', 'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold', 'other_weather', 'direct_report']\n"
     ]
    }
   ],
   "source": [
    "# View the first few rows of the DataFrame\n",
    "display(df.head(3))\n",
    "display_dataset(X, Y)\n",
    "classes = response_labels.columns\n",
    "print(f\"\\nlabels: {list(classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/emmanuele/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords and all nltk relevant packages.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\('\n",
      "/var/folders/w4/j1dy6gvs5mv5_x5pw5lfbn8r0000gp/T/ipykernel_936/4262558252.py:18: SyntaxWarning: invalid escape sequence '\\('\n",
      "  url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text, stop_words=None):\n",
    "    \"\"\"\n",
    "    Tokenize a text by normalizing, lemmatizing and removing stop words.\n",
    "    \n",
    "    Args:\n",
    "        text (list): list of strings\n",
    "        stop_words (set): a set of word strings for stop words.\n",
    "\n",
    "    Returns:\n",
    "        tokens(list): list of token strings.\n",
    "    \"\"\"\n",
    "    # Import stopwords if not imported.\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()    \n",
    "    \n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Replace URLs with a placeholder and normalize case.\n",
    "    normalized_text = re.sub(url_regex, ' ', text.lower())\n",
    "\n",
    "    # Replace non-alphanumeric characters with spaces.\n",
    "    normalized_text = re.sub(r'[^a-zA-Z0-9]', ' ', normalized_text)\n",
    "    \n",
    "    tokens = word_tokenize(normalized_text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Consider extending the tokenize function to be able to perform sentence tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: \"Barclaysjbki CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  https://www.google.com\"\n",
      "\n",
      "text tokens: ['barclaysjbki', 'ceo', 'stress', 'importance', 'regulatory', 'cultural', 'reform', 'financial', 'service', 'brussels', 'conference'] \n",
      "\n",
      "input text: \"The No. 8 Northeast Gale or storm signal was issued at 5.55pm yesterday (September 14) and was replaced by Southeast gale and storm signal at 12.35am today (September 15).\" \n",
      "\n",
      "text tokens: ['8', 'northeast', 'gale', 'storm', 'signal', 'issued', '5', '55pm', 'yesterday', 'september', '14', 'replaced', 'southeast', 'gale', 'storm', 'signal', '12', '35am', 'today', 'september', '15'] \n",
      "\n",
      "sentences: ['The No.', '8 Northeast Gale or storm signal was issued at 5.55pm yesterday (September 14) and was replaced by Southeast gale and storm signal at 12.35am today (September 15).'] \n",
      "\n",
      "testing sentence tokenization...\n",
      "\n",
      "text: \"The No.\"\n",
      "\n",
      "text tokens: []\n",
      "\n",
      "text: \"8 Northeast Gale or storm signal was issued at 5.55pm yesterday (September 14) and was replaced by Southeast gale and storm signal at 12.35am today (September 15).\"\n",
      "\n",
      "text tokens: ['8', 'northeast', 'gale', 'storm', 'signal', 'issued', '5', '55pm', 'yesterday', 'september', '14', 'replaced', 'southeast', 'gale', 'storm', 'signal', '12', '35am', 'today', 'september', '15']\n"
     ]
    }
   ],
   "source": [
    "#Testing Tokenize Function\n",
    "\n",
    "text1 = \"Barclaysjbki CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  https://www.google.com\"\n",
    "print(f'input text: \"{text1}\"\\n')\n",
    "print(f\"text tokens: {tokenize(text1)} \\n\")\n",
    "text2 = \"The No. 8 Northeast Gale or storm signal was issued at 5.55pm yesterday (September 14) and was replaced by Southeast gale and storm signal at 12.35am today (September 15).\"\n",
    "print(f'input text: \"{text2}\" \\n')\n",
    "print(f\"text tokens: {tokenize(text2)} \\n\")\n",
    "sentence_list = sent_tokenize(text2)\n",
    "print(f\"sentences: {sentence_list} \\n\")\n",
    "print(\"testing sentence tokenization...\")\n",
    "for text in sentence_list:\n",
    "    print(f'\\ntext: \"{text}\"')\n",
    "    print(f\"\\ntext tokens: {tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "what\n",
      "what, True\n",
      "can\n",
      "can, True\n",
      "i\n",
      "i, True\n",
      "do\n",
      "do, True\n",
      "?\n",
      "?, False\n"
     ]
    }
   ],
   "source": [
    "text = 'What can I do?'\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "for token in word_tokenize(text.lower()):\n",
    "    print(WordNetLemmatizer().lemmatize(token))\n",
    "    print(f'{token}, {token in set(stopwords.words(\"english\"))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "#help(sklearn.multioutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique Y values:  [0 1 2]\n",
      "training set, X:  (21108,)\n",
      "test set, X:  (5278,)\n",
      "training set, Y:  (21108, 36)\n",
      "test set, Y:  (5278, 36)\n",
      "\n",
      "\n",
      "X1 shape:  (21108,)\n",
      "X2 shape:  (21108, 36)\n",
      "X1 Type:  <class 'numpy.ndarray'>\n",
      "X2 Type:  <class 'numpy.ndarray'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "display_dataset(X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test)\n",
    "print(\"\\n\")\n",
    "print(data_type_check(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer(tokenizer=tokenize)),\n",
    "    (\"tfidf\", TfidfTransformer()),\n",
    "    (\"clf\", RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trained_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#help(sklearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trained_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9450181045008631\n",
      "0.9450181045008631\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_actual, y_pred):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        y_actual (_type_): 1D or ND arrary for labelled y/ output examples\n",
    "        y_pred (_type_): 1D or ND arrary for predicted/inference output examples\n",
    "    \"\"\"\n",
    "    accuracy = (y_pred == y_actual).mean()\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_multilabel_model(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a multi-label classification model.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth (correct) target values\n",
    "        y_pred: Estimated targets as returned by classifier\n",
    "        class_names: List of class names for each column\n",
    "    \"\"\"\n",
    "    # Overall accuracy\n",
    "    sample_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Overall Sample-wise Accuracy: {sample_accuracy:.3f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    results = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        precision = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        recall = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        \n",
    "        results.append({\n",
    "            'Class': class_names[i],\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        # Detailed classification report per class\n",
    "        print(f\"\\nDetailed metrics for {class_names[i]}:\")\n",
    "        print(classification_report(y_true[:, i], y_pred[:, i], zero_division=0))\n",
    "    \n",
    "    # Create a DataFrame with all metrics\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    print(\"\\nSummary of all metrics:\")\n",
    "    print(metrics_df.round(3))\n",
    "    \n",
    "    # Visualize metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics_melted = pd.melt(metrics_df, id_vars=['Class'], \n",
    "                           value_vars=['Precision', 'Recall', 'F1-Score'])\n",
    "    sns.barplot(x='Class', y='value', hue='variable', data=metrics_melted)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Model Performance Metrics by Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Example usage:\n",
    "def evaluate_model(model, X_test, y_test, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate the model and print all relevant metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get best parameters if using GridSearchCV\n",
    "    if hasattr(model, 'best_params_'):\n",
    "        print(\"Best parameters found:\")\n",
    "        print(model.best_params_)\n",
    "        print(\"\\nBest cross-validation score:\", model.best_score_)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics_df = evaluate_multilabel_model(y_test, y_pred, class_names)\n",
    "    \n",
    "    return y_pred, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[337], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m(new_model, X_test, y_test, classes)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy(y_test, y_pred))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Now you can generate the classification report\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_model(new_model, X_test, y_test, classes)\n",
    "\n",
    "print(accuracy(y_test, y_pred))\n",
    "\n",
    "# Now you can generate the classification report\n",
    "for col_index in range(0,y_test.shape[1]):\n",
    "    report = classification_report(y_test[:,col_index], y_pred[:, col_index], zero_division=0)\n",
    "    print(classes[col_index])\n",
    "    print(report)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(sklearn)\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion,Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"_summary_\n",
    "        \"\"\"\n",
    "        self.run_count = 1\n",
    "        print(\"Start Verb Extractor running...\")\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        try:\n",
    "            \n",
    "            # print(\"\\n\\nText:\", text)\n",
    "            sentence_list = nltk.sent_tokenize(text)\n",
    "            for sentence in sentence_list:\n",
    "                # Tokenize the sentence\n",
    "                text_tokens = tokenize(sentence)\n",
    "                \n",
    "                if text_tokens:\n",
    "                    # Get the POS (parts of speech) of the words in the text.\n",
    "                    first_word, first_tag = nltk.pos_tag(text_tokens)[0]\n",
    "                    \n",
    "                    # Check if the first word is a Verb or 'RT' (retweet). \n",
    "                    if first_tag in ['VB', 'VBP', 'UH'] or first_word == 'RT':\n",
    "                        # print(\"\\nVerb, Tag: \",first_tag, \", First Word: \",first_word)\n",
    "                        return 1 \n",
    "                    else:\n",
    "                        # print(f\"\\nNon-verb, Tag: {first_tag}, First Word: ,{first_word}\")\n",
    "                        return 0\n",
    "                else:\n",
    "                    # print(f'Empty Text Tokens, {text_tokens} in \"{sentence}\"')\n",
    "                    pass\n",
    "            \n",
    "            # If no sentences were found in the entire text.\n",
    "            self.run_count += 1\n",
    "            print(f\"Empty Text ({self.run_count}): \", sentence_list)\n",
    "            return 0\n",
    "        \n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError: {e}\")\n",
    "            print(f\"Text causing issue: {text}\")\n",
    "            return 0\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            print(f\"Text causing issue: {text}\")\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        df = pd.DataFrame(X_tagged).values # converts to a 2D numpy array.\n",
    "        # df = X_tagged.values # removing this because the hstack fails.\n",
    "        \n",
    "        # Log information about the transformation\n",
    "        print(\"\\n\\nFeature Extraction and Text Transformation Complete:\")\n",
    "        print(\"Extracted/New feature shape:\", df.shape)\n",
    "        print(\"Input feature shape: \", X.shape)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_pipeline():\n",
    "    \"\"\"\n",
    "    Constructs an ML pipeline using FeatureUnion to add a new binary feature.\n",
    "    The feature checks if the first word in each text is a verb (1 if a verb, 0 otherwise),\n",
    "    and combines this feature with the text data processed through a TF-IDF transformer pipeline.\n",
    "    The combined feature matrix is used to train the model to enhance its performance.\n",
    "\n",
    "    Returns:\n",
    "        model (Pipeline): A scikit-learn pipeline object for training and evaluation, \n",
    "        which includes feature extraction, transformation, and a classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_pipeline = Pipeline([\n",
    "        (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer())\n",
    "    ])\n",
    "    \n",
    "    features = FeatureUnion([\n",
    "        (\"text_pipeline\",text_pipeline),\n",
    "        (\"starting_verb\", StartingVerbExtractor())\n",
    "    ])\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"features\", features),\n",
    "        (\"clf\", MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "    \n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'clf__estimator__n_estimators': [10, 25, 50],\n",
    "        'clf__estimator__min_samples_split': [2, 3, 4]\n",
    "    }\n",
    "    \n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(estimator=model, param_grid=parameters, verbose=2, cv=3, error_score='raise')\n",
    "    \n",
    "    return cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique Y values:  [0 1 2]\n",
      "training set, X:  (21108,)\n",
      "test set, X:  (5278,)\n",
      "training set, Y:  (21108, 36)\n",
      "test set, Y:  (5278, 36)\n",
      "\n",
      "\n",
      "X1 shape:  (21108,)\n",
      "X2 shape:  (21108, 36)\n",
      "X1 Type:  <class 'numpy.ndarray'>\n",
      "X2 Type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "display_dataset(X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test)\n",
    "print(\"\\n\")\n",
    "data_type_check(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Verb Extractor running...\n",
      "Start Verb Extractor running...\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  21.2s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  20.6s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  21.2s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  32.3s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  33.2s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  31.9s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  37.5s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  37.9s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  38.6s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.2min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.1min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.1min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time= 1.1min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time= 1.1min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time= 1.1min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 2.1min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 2.2min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 2.2min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  20.4s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  21.3s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  19.4s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  29.9s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  30.2s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  31.4s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  37.1s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  40.8s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  34.4s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time=  58.0s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.0min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time=  57.8s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time= 1.0min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time=  59.8s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time= 1.0min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.9min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.9min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=3, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.9min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  19.7s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  18.9s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 1); total time=  19.7s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  28.9s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  30.3s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=10, features__text_pipeline__vect__ngram_range=(1, 2); total time=  29.4s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  33.7s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  35.6s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 1); total time=  38.6s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time=  55.6s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time=  54.5s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=25, features__text_pipeline__vect__ngram_range=(1, 2); total time=  52.8s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time=  56.7s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time=  56.5s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 1); total time=  56.9s\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.9min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['          .']\n",
      "Empty Text (3):  ['//// // @:@']\n",
      "Empty Text (4):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (5):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (6):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 2.0min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (14072, 1)\n",
      "Input feature shape:  (14072,)\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (7036, 1)\n",
      "Input feature shape:  (7036,)\n",
      "[CV] END clf__estimator__min_samples_split=4, clf__estimator__n_estimators=50, features__text_pipeline__vect__ngram_range=(1, 2); total time= 1.7min\n",
      "Start Verb Extractor running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuele/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (2):  ['(http://www.guardian.co.uk/global-development/2013/jan/16/somali-ngos-mogadishu-street-children)']\n",
      "Empty Text (3):  []\n",
      "Empty Text (4):  ['          .']\n",
      "Empty Text (5):  ['//// // @:@']\n",
      "Empty Text (6):  ['http://wap.sina.comhttp://wap.sina.com']\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (21108, 1)\n",
      "Input feature shape:  (21108,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-13 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-13 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-13 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-13 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-13 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-13 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-13 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-13 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-13 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-13 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, error_score=&#x27;raise&#x27;,\n",
       "             estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(tokenizer=&lt;function tokenize at 0x4094c0e00&gt;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfTransformer())])),\n",
       "                                                                       (&#x27;starting_verb&#x27;,\n",
       "                                                                        StartingVerbExtractor())])),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        MultiOutputClassifier(estimator=RandomForestClassifier()))]),\n",
       "             param_grid={&#x27;clf__estimator__min_samples_split&#x27;: [2, 3, 4],\n",
       "                         &#x27;clf__estimator__n_estimators&#x27;: [10, 25, 50],\n",
       "                         &#x27;features__text_pipeline__vect__ngram_range&#x27;: ((1, 1),\n",
       "                                                                        (1,\n",
       "                                                                         2))},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-97\" type=\"checkbox\" ><label for=\"sk-estimator-id-97\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=3, error_score=&#x27;raise&#x27;,\n",
       "             estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                                         CountVectorizer(tokenizer=&lt;function tokenize at 0x4094c0e00&gt;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfTransformer())])),\n",
       "                                                                       (&#x27;starting_verb&#x27;,\n",
       "                                                                        StartingVerbExtractor())])),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        MultiOutputClassifier(estimator=RandomForestClassifier()))]),\n",
       "             param_grid={&#x27;clf__estimator__min_samples_split&#x27;: [2, 3, 4],\n",
       "                         &#x27;clf__estimator__n_estimators&#x27;: [10, 25, 50],\n",
       "                         &#x27;features__text_pipeline__vect__ngram_range&#x27;: ((1, 1),\n",
       "                                                                        (1,\n",
       "                                                                         2))},\n",
       "             verbose=2)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-98\" type=\"checkbox\" ><label for=\"sk-estimator-id-98\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=&lt;function tokenize at 0x4094c0e00&gt;)),\n",
       "                                                                 (&#x27;tfidf&#x27;,\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                (&#x27;starting_verb&#x27;,\n",
       "                                                 StartingVerbExtractor())])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(n_estimators=50)))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-99\" type=\"checkbox\" ><label for=\"sk-estimator-id-99\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;features: FeatureUnion<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.FeatureUnion.html\">?<span>Documentation for features: FeatureUnion</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>FeatureUnion(transformer_list=[(&#x27;text_pipeline&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                                 CountVectorizer(ngram_range=(1,\n",
       "                                                                              2),\n",
       "                                                                 tokenizer=&lt;function tokenize at 0x4094c0e00&gt;)),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfTransformer())])),\n",
       "                               (&#x27;starting_verb&#x27;, StartingVerbExtractor())])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>text_pipeline</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-100\" type=\"checkbox\" ><label for=\"sk-estimator-id-100\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(ngram_range=(1, 2),\n",
       "                tokenizer=&lt;function tokenize at 0x4094c0e00&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-101\" type=\"checkbox\" ><label for=\"sk-estimator-id-101\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfTransformer()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>starting_verb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-102\" type=\"checkbox\" ><label for=\"sk-estimator-id-102\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">StartingVerbExtractor</label><div class=\"sk-toggleable__content fitted\"><pre>StartingVerbExtractor()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-103\" type=\"checkbox\" ><label for=\"sk-estimator-id-103\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;clf: MultiOutputClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for clf: MultiOutputClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=RandomForestClassifier(n_estimators=50))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-104\" type=\"checkbox\" ><label for=\"sk-estimator-id-104\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=50)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-105\" type=\"checkbox\" ><label for=\"sk-estimator-id-105\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=50)</pre></div> </div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "             estimator=Pipeline(steps=[('features',\n",
       "                                        FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                                        Pipeline(steps=[('vect',\n",
       "                                                                                         CountVectorizer(tokenizer=<function tokenize at 0x4094c0e00>)),\n",
       "                                                                                        ('tfidf',\n",
       "                                                                                         TfidfTransformer())])),\n",
       "                                                                       ('starting_verb',\n",
       "                                                                        StartingVerbExtractor())])),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=RandomForestClassifier()))]),\n",
       "             param_grid={'clf__estimator__min_samples_split': [2, 3, 4],\n",
       "                         'clf__estimator__n_estimators': [10, 25, 50],\n",
       "                         'features__text_pipeline__vect__ngram_range': ((1, 1),\n",
       "                                                                        (1,\n",
       "                                                                         2))},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = create_model_pipeline()\n",
    "new_model.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters found: \", new_model.best_params_)\n",
    "# print(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cv', 'error_score', 'estimator__memory', 'estimator__steps', 'estimator__verbose', 'estimator__features', 'estimator__clf', 'estimator__features__n_jobs', 'estimator__features__transformer_list', 'estimator__features__transformer_weights', 'estimator__features__verbose', 'estimator__features__verbose_feature_names_out', 'estimator__features__text_pipeline', 'estimator__features__starting_verb', 'estimator__features__text_pipeline__memory', 'estimator__features__text_pipeline__steps', 'estimator__features__text_pipeline__verbose', 'estimator__features__text_pipeline__vect', 'estimator__features__text_pipeline__tfidf', 'estimator__features__text_pipeline__vect__analyzer', 'estimator__features__text_pipeline__vect__binary', 'estimator__features__text_pipeline__vect__decode_error', 'estimator__features__text_pipeline__vect__dtype', 'estimator__features__text_pipeline__vect__encoding', 'estimator__features__text_pipeline__vect__input', 'estimator__features__text_pipeline__vect__lowercase', 'estimator__features__text_pipeline__vect__max_df', 'estimator__features__text_pipeline__vect__max_features', 'estimator__features__text_pipeline__vect__min_df', 'estimator__features__text_pipeline__vect__ngram_range', 'estimator__features__text_pipeline__vect__preprocessor', 'estimator__features__text_pipeline__vect__stop_words', 'estimator__features__text_pipeline__vect__strip_accents', 'estimator__features__text_pipeline__vect__token_pattern', 'estimator__features__text_pipeline__vect__tokenizer', 'estimator__features__text_pipeline__vect__vocabulary', 'estimator__features__text_pipeline__tfidf__norm', 'estimator__features__text_pipeline__tfidf__smooth_idf', 'estimator__features__text_pipeline__tfidf__sublinear_tf', 'estimator__features__text_pipeline__tfidf__use_idf', 'estimator__clf__estimator__bootstrap', 'estimator__clf__estimator__ccp_alpha', 'estimator__clf__estimator__class_weight', 'estimator__clf__estimator__criterion', 'estimator__clf__estimator__max_depth', 'estimator__clf__estimator__max_features', 'estimator__clf__estimator__max_leaf_nodes', 'estimator__clf__estimator__max_samples', 'estimator__clf__estimator__min_impurity_decrease', 'estimator__clf__estimator__min_samples_leaf', 'estimator__clf__estimator__min_samples_split', 'estimator__clf__estimator__min_weight_fraction_leaf', 'estimator__clf__estimator__monotonic_cst', 'estimator__clf__estimator__n_estimators', 'estimator__clf__estimator__n_jobs', 'estimator__clf__estimator__oob_score', 'estimator__clf__estimator__random_state', 'estimator__clf__estimator__verbose', 'estimator__clf__estimator__warm_start', 'estimator__clf__estimator', 'estimator__clf__n_jobs', 'estimator', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(new_model.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking outputs of Transformers\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FeatureUnion' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[340], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, transformer \u001b[38;5;129;01min\u001b[39;00m new_model\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39msteps:\n\u001b[1;32m     10\u001b[0m     output \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FeatureUnion' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "if isinstance(new_model, Pipeline):\n",
    "    print(\"Best parameters found: \", new_model.best_params_ if hasattr(new_model, 'best_params_') else \"No best parameters found.\")\n",
    "    print(new_model)\n",
    "    y_pred = new_model.predict(X_test)\n",
    "    print(\"y_pred:\", y_pred)\n",
    "\n",
    "if isinstance(new_model, GridSearchCV):\n",
    "    print('Checking outputs of Transformers')\n",
    "    for name, transformer in new_model.best_estimator_.steps:\n",
    "        output = transformer.fit(X_train, y_train)\n",
    "        print(f\"{name} output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def inspect_pipeline_steps(model, X_train):\n",
    "    \"\"\"\n",
    "    Safely inspect each step of a fitted pipeline with proper error handling\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted GridSearchCV object\n",
    "        X_train: Training data to inspect transformations\n",
    "    \"\"\"\n",
    "    if not isinstance(model, GridSearchCV):\n",
    "        print(\"Warning: Model is not a GridSearchCV instance\")\n",
    "        return\n",
    "    \n",
    "    print(\"Best Parameters:\", model.best_params_)\n",
    "    print(\"\\nInspecting Pipeline Steps:\")\n",
    "    \n",
    "    # Access the fitted pipeline\n",
    "    pipeline = model.best_estimator_\n",
    "    current_data = X_train\n",
    "    \n",
    "    # Iterate through named steps\n",
    "    for name, transformer in pipeline.named_steps.items():\n",
    "        print(f\"\\nStep: {name}\")\n",
    "        print(f\"Input shape: {current_data.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if step has transform method\n",
    "            if hasattr(transformer, 'transform'):\n",
    "                current_data = transformer.transform(current_data)\n",
    "                print(f\"Output shape: {current_data.shape}\")\n",
    "                \n",
    "                # Print feature names if available\n",
    "                if hasattr(transformer, 'get_feature_names_out'):\n",
    "                    features = transformer.get_feature_names_out()\n",
    "                    print(f\"First few features: {features[:5]}\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"Note: {name} doesn't have transform method (might be final estimator)\")\n",
    "                \n",
    "            # Print additional info for specific transformer types\n",
    "            if hasattr(transformer, 'n_features_in_'):\n",
    "                print(f\"Number of input features: {transformer.n_features_in_}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return current_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_multilabel_model(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a multi-label classification model.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth (correct) target values\n",
    "        y_pred: Estimated targets as returned by classifier\n",
    "        class_names: List of class names for each column\n",
    "    \"\"\"\n",
    "    # Overall accuracy\n",
    "    # sample_accuracy = accuracy_score(y_true, y_pred)\n",
    "    # print(f\"Overall Sample-wise Accuracy: {sample_accuracy:.3f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    results = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        precision = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        recall = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_true[:, i], y_pred[:, i], average='macro', zero_division=0)\n",
    "        \n",
    "        results.append({\n",
    "            'Class': class_names[i],\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "        \n",
    "        # Detailed classification report per class\n",
    "        print(f\"\\nDetailed metrics for {class_names[i]}:\")\n",
    "        print(classification_report(y_true[:, i], y_pred[:, i], zero_division=0))\n",
    "    \n",
    "    # Create a DataFrame with all metrics\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    print(\"\\nSummary of all metrics:\")\n",
    "    print(metrics_df.round(3))\n",
    "    \n",
    "    # Visualize metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics_melted = pd.melt(metrics_df, id_vars=['Class'], \n",
    "                           value_vars=['Precision', 'Recall', 'F1-Score'])\n",
    "    sns.barplot(x='Class', y='value', hue='variable', data=metrics_melted)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Model Performance Metrics by Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Example usage:\n",
    "def evaluate_model(model, X_test, y_test, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate the model and print all relevant metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get best parameters if using GridSearchCV\n",
    "    if hasattr(model, 'best_params_'):\n",
    "        print(\"Best parameters found:\")\n",
    "        print(model.best_params_)\n",
    "        print(\"\\nBest cross-validation score:\", model.best_score_)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics_df = evaluate_multilabel_model(y_test, y_pred, class_names)\n",
    "    \n",
    "    return y_pred, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Text (13):  []\n",
      "\n",
      "\n",
      "Feature Extraction and Text Transformation Complete:\n",
      "Extracted/New feature shape: (5278, 1)\n",
      "Input feature shape:  (5278,)\n",
      "Best parameters found:\n",
      "{'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 50, 'features__text_pipeline__vect__ngram_range': (1, 2)}\n",
      "\n",
      "Best cross-validation score: 0.26658139094182304\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[349], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[348], line 70\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, y_test, class_names)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest cross-validation score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_multilabel_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred, metrics_df\n",
      "Cell \u001b[0;32mIn[348], line 23\u001b[0m, in \u001b[0;36mevaluate_multilabel_model\u001b[0;34m(y_true, y_pred, class_names)\u001b[0m\n\u001b[1;32m     21\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 23\u001b[0m     precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     recall \u001b[38;5;241m=\u001b[39m recall_score(y_true[:, i], y_pred[:, i], zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(y_true[:, i], y_pred[:, i], zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2190\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2024\u001b[0m     {\n\u001b[1;32m   2025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2051\u001b[0m ):\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   2053\u001b[0m \n\u001b[1;32m   2054\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2190\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2197\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1775\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1775\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/ML/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1564\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1563\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1564\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1565\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1566\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[1;32m   1567\u001b[0m         )\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1569\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1570\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1575\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "evaluate_model(new_model, X_test, y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.45      0.55      1183\n",
      "           1       0.85      0.95      0.90      4054\n",
      "           2       0.46      0.39      0.42        41\n",
      "\n",
      "    accuracy                           0.83      5278\n",
      "   macro avg       0.67      0.59      0.62      5278\n",
      "weighted avg       0.82      0.83      0.82      5278\n",
      "\n",
      "request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      4361\n",
      "           1       0.82      0.50      0.62       917\n",
      "\n",
      "    accuracy                           0.89      5278\n",
      "   macro avg       0.86      0.74      0.78      5278\n",
      "weighted avg       0.89      0.89      0.88      5278\n",
      "\n",
      "offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5258\n",
      "           1       1.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           1.00      5278\n",
      "   macro avg       1.00      0.50      0.50      5278\n",
      "weighted avg       1.00      1.00      0.99      5278\n",
      "\n",
      "aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82      3075\n",
      "           1       0.76      0.71      0.74      2203\n",
      "\n",
      "    accuracy                           0.79      5278\n",
      "   macro avg       0.78      0.78      0.78      5278\n",
      "weighted avg       0.79      0.79      0.79      5278\n",
      "\n",
      "medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      4859\n",
      "           1       0.64      0.10      0.17       419\n",
      "\n",
      "    accuracy                           0.92      5278\n",
      "   macro avg       0.78      0.55      0.56      5278\n",
      "weighted avg       0.90      0.92      0.90      5278\n",
      "\n",
      "medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      5003\n",
      "           1       0.77      0.09      0.16       275\n",
      "\n",
      "    accuracy                           0.95      5278\n",
      "   macro avg       0.86      0.54      0.57      5278\n",
      "weighted avg       0.94      0.95      0.93      5278\n",
      "\n",
      "search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      5119\n",
      "           1       0.86      0.04      0.07       159\n",
      "\n",
      "    accuracy                           0.97      5278\n",
      "   macro avg       0.91      0.52      0.53      5278\n",
      "weighted avg       0.97      0.97      0.96      5278\n",
      "\n",
      "security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5172\n",
      "           1       1.00      0.02      0.04       106\n",
      "\n",
      "    accuracy                           0.98      5278\n",
      "   macro avg       0.99      0.51      0.51      5278\n",
      "weighted avg       0.98      0.98      0.97      5278\n",
      "\n",
      "military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5086\n",
      "           1       0.86      0.03      0.06       192\n",
      "\n",
      "    accuracy                           0.96      5278\n",
      "   macro avg       0.91      0.52      0.52      5278\n",
      "weighted avg       0.96      0.96      0.95      5278\n",
      "\n",
      "child_alone\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5278\n",
      "\n",
      "    accuracy                           1.00      5278\n",
      "   macro avg       1.00      1.00      1.00      5278\n",
      "weighted avg       1.00      1.00      1.00      5278\n",
      "\n",
      "water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      4948\n",
      "           1       0.90      0.41      0.56       330\n",
      "\n",
      "    accuracy                           0.96      5278\n",
      "   macro avg       0.93      0.70      0.77      5278\n",
      "weighted avg       0.96      0.96      0.95      5278\n",
      "\n",
      "food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      4700\n",
      "           1       0.83      0.63      0.72       578\n",
      "\n",
      "    accuracy                           0.95      5278\n",
      "   macro avg       0.89      0.81      0.84      5278\n",
      "weighted avg       0.94      0.95      0.94      5278\n",
      "\n",
      "shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      4790\n",
      "           1       0.82      0.41      0.54       488\n",
      "\n",
      "    accuracy                           0.94      5278\n",
      "   macro avg       0.88      0.70      0.75      5278\n",
      "weighted avg       0.93      0.94      0.93      5278\n",
      "\n",
      "clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5198\n",
      "           1       0.67      0.07      0.13        80\n",
      "\n",
      "    accuracy                           0.99      5278\n",
      "   macro avg       0.83      0.54      0.56      5278\n",
      "weighted avg       0.98      0.99      0.98      5278\n",
      "\n",
      "money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5161\n",
      "           1       0.83      0.04      0.08       117\n",
      "\n",
      "    accuracy                           0.98      5278\n",
      "   macro avg       0.91      0.52      0.54      5278\n",
      "weighted avg       0.98      0.98      0.97      5278\n",
      "\n",
      "missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5221\n",
      "           1       1.00      0.05      0.10        57\n",
      "\n",
      "    accuracy                           0.99      5278\n",
      "   macro avg       0.99      0.53      0.55      5278\n",
      "weighted avg       0.99      0.99      0.99      5278\n",
      "\n",
      "refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5091\n",
      "           1       0.60      0.03      0.06       187\n",
      "\n",
      "    accuracy                           0.96      5278\n",
      "   macro avg       0.78      0.52      0.52      5278\n",
      "weighted avg       0.95      0.96      0.95      5278\n",
      "\n",
      "death\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5048\n",
      "           1       0.86      0.16      0.27       230\n",
      "\n",
      "    accuracy                           0.96      5278\n",
      "   macro avg       0.91      0.58      0.63      5278\n",
      "weighted avg       0.96      0.96      0.95      5278\n",
      "\n",
      "other_aid\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      4571\n",
      "           1       0.66      0.05      0.09       707\n",
      "\n",
      "    accuracy                           0.87      5278\n",
      "   macro avg       0.77      0.52      0.51      5278\n",
      "weighted avg       0.84      0.87      0.82      5278\n",
      "\n",
      "infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      4925\n",
      "           1       0.33      0.00      0.01       353\n",
      "\n",
      "    accuracy                           0.93      5278\n",
      "   macro avg       0.63      0.50      0.49      5278\n",
      "weighted avg       0.89      0.93      0.90      5278\n",
      "\n",
      "transport\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5042\n",
      "           1       0.80      0.12      0.21       236\n",
      "\n",
      "    accuracy                           0.96      5278\n",
      "   macro avg       0.88      0.56      0.59      5278\n",
      "weighted avg       0.95      0.96      0.94      5278\n",
      "\n",
      "buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5009\n",
      "           1       0.80      0.15      0.25       269\n",
      "\n",
      "    accuracy                           0.95      5278\n",
      "   macro avg       0.88      0.57      0.61      5278\n",
      "weighted avg       0.95      0.95      0.94      5278\n",
      "\n",
      "electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5173\n",
      "           1       0.83      0.05      0.09       105\n",
      "\n",
      "    accuracy                           0.98      5278\n",
      "   macro avg       0.91      0.52      0.54      5278\n",
      "weighted avg       0.98      0.98      0.97      5278\n",
      "\n",
      "tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5243\n",
      "           1       1.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.99      5278\n",
      "   macro avg       1.00      0.50      0.50      5278\n",
      "weighted avg       0.99      0.99      0.99      5278\n",
      "\n",
      "hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5211\n",
      "           1       1.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.99      5278\n",
      "   macro avg       0.99      0.50      0.50      5278\n",
      "weighted avg       0.99      0.99      0.98      5278\n",
      "\n",
      "shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5255\n",
      "           1       1.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           1.00      5278\n",
      "   macro avg       1.00      0.50      0.50      5278\n",
      "weighted avg       1.00      1.00      0.99      5278\n",
      "\n",
      "aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5209\n",
      "           1       1.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.99      5278\n",
      "   macro avg       0.99      0.50      0.50      5278\n",
      "weighted avg       0.99      0.99      0.98      5278\n",
      "\n",
      "other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5058\n",
      "           1       0.25      0.00      0.01       220\n",
      "\n",
      "    accuracy                           0.96      5278\n",
      "   macro avg       0.60      0.50      0.49      5278\n",
      "weighted avg       0.93      0.96      0.94      5278\n",
      "\n",
      "weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      3782\n",
      "           1       0.85      0.71      0.77      1496\n",
      "\n",
      "    accuracy                           0.88      5278\n",
      "   macro avg       0.87      0.83      0.85      5278\n",
      "weighted avg       0.88      0.88      0.88      5278\n",
      "\n",
      "floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4824\n",
      "           1       0.94      0.45      0.61       454\n",
      "\n",
      "    accuracy                           0.95      5278\n",
      "   macro avg       0.95      0.73      0.79      5278\n",
      "weighted avg       0.95      0.95      0.94      5278\n",
      "\n",
      "storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      4773\n",
      "           1       0.79      0.53      0.64       505\n",
      "\n",
      "    accuracy                           0.94      5278\n",
      "   macro avg       0.87      0.76      0.80      5278\n",
      "weighted avg       0.94      0.94      0.94      5278\n",
      "\n",
      "fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5219\n",
      "           1       0.67      0.03      0.06        59\n",
      "\n",
      "    accuracy                           0.99      5278\n",
      "   macro avg       0.83      0.52      0.53      5278\n",
      "weighted avg       0.99      0.99      0.98      5278\n",
      "\n",
      "earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      4788\n",
      "           1       0.88      0.79      0.83       490\n",
      "\n",
      "    accuracy                           0.97      5278\n",
      "   macro avg       0.93      0.89      0.91      5278\n",
      "weighted avg       0.97      0.97      0.97      5278\n",
      "\n",
      "cold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5158\n",
      "           1       0.62      0.04      0.08       120\n",
      "\n",
      "    accuracy                           0.98      5278\n",
      "   macro avg       0.80      0.52      0.53      5278\n",
      "weighted avg       0.97      0.98      0.97      5278\n",
      "\n",
      "other_weather\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4992\n",
      "           1       0.53      0.03      0.06       286\n",
      "\n",
      "    accuracy                           0.95      5278\n",
      "   macro avg       0.74      0.51      0.52      5278\n",
      "weighted avg       0.92      0.95      0.92      5278\n",
      "\n",
      "direct_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92      4255\n",
      "           1       0.78      0.38      0.51      1023\n",
      "\n",
      "    accuracy                           0.86      5278\n",
      "   macro avg       0.83      0.68      0.72      5278\n",
      "weighted avg       0.85      0.86      0.84      5278\n",
      "\n",
      "Accuracy:  0.94879689276241\n",
      "0.94879689276241\n"
     ]
    }
   ],
   "source": [
    "for col_index in range(0, y_test.shape[1]):\n",
    "    print(classes[col_index])\n",
    "    report = classification_report(y_test[:,col_index], y_pred[:,col_index], zero_division=1)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "print(accuracy(y_test, y_pred))    \n",
    "\n",
    "\n",
    "def model_metrics():\n",
    "    \"\"\"\n",
    "    Custom function to evaluate performance of ml models\n",
    "    Accuracy, Recall & Precision.\n",
    "    \n",
    "    Args\n",
    "    Returns:\n",
    "        None, \n",
    "    \"\"\"\n",
    "    accuracy = accuracy(y_actual, y_pred)\n",
    "    if labels:\n",
    "        confusion_matrix = confusion_matrix(y_actual, y_pred, labels=labels)\n",
    "    else:\n",
    "        confusion_matrix = confusion_matrix(y_actual, y_pred)\n",
    "    print(confusion_matrix)\n",
    "    return accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    \"\"\"\n",
    "    Constructs an ML pipeline using FeatureUnion to add a new binary feature.\n",
    "    The feature checks if the first word in each text is a verb (1 if a verb, 0 otherwise),\n",
    "    and combines this feature with the text data processed through a TF-IDF transformer pipeline.\n",
    "    The combined feature matrix is used to train the model to enhance its performance.\n",
    "\n",
    "    Returns:\n",
    "        model (Pipeline): A scikit-learn pipeline object for training and evaluation, \n",
    "        which includes feature extraction, transformation, and a classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_pipeline = Pipeline([\n",
    "        (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer())\n",
    "    ])\n",
    "    \n",
    "    features = FeatureUnion([\n",
    "        (\"text_pipeline\",text_pipeline),\n",
    "        (\"starting_verb\", StartingVerbExtractor())\n",
    "    ])\n",
    "    \n",
    "    model = Pipeline([\n",
    "        (\"features\", features),\n",
    "        (\"clf\", RandomBoostedTrees())\n",
    "])\n",
    "    \n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'clf__n_estimators': [1, 25, 125],\n",
    "        'clf__max_depth': [None, 25, 125]\n",
    "    } \n",
    "    \n",
    "    # create grid search object\n",
    "    # cv = GridSearchCV(estimator=pipeline, param_grid=parameters, verbose=2, cv=3, error_score='raise' )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Parameters\n",
    "\n",
    "\n",
    "# parameters = {\n",
    "#         'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#         'clf__n_estimators': [50, 100, 200],\n",
    "#         'clf__min_samples_split': [2, 3, 4]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "# Step 2: Export the model using pickle\n",
    "with open('train_classifier.pkl', 'wb') as model_file:\n",
    "    pickle.dump(new_model, model_file)\n",
    "\n",
    "print(\"Model exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#     \"\"\"\n",
    "#     Model.\n",
    "    \n",
    "#     Args\n",
    "    \n",
    "#     \"\"\"\n",
    "#     pipeline = Pipeline([\n",
    "#     ('features', FeatureUnion([\n",
    "#         ('text_pipeline', Pipeline([\n",
    "#             ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#             ('tfidf', TfidfTransformer())\n",
    "#         ])),\n",
    "#         ('starting_verb', StartingVerbExtractor())\n",
    "#     ])),\n",
    "#     ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "#     ])\n",
    "            \n",
    "#     # specify parameters for grid search\n",
    "#     parameters = {\n",
    "#         'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#         'clf__estimator__n_estimators': [10, 25, 50],\n",
    "#         'clf__estimator__min_samples_split': [2, 3, 4]\n",
    "#     }\n",
    "    \n",
    "#     # create grid search object\n",
    "#     # cv = GridSearchCV(estimator=pipeline, param_grid=parameters, verbose=2, cv=3, error_score='raise' )\n",
    "    \n",
    "#     return pipeline\n",
    "\n",
    "\n",
    "\n",
    "# def build_model():\n",
    "#     \"\"\"\n",
    "#     Model.\n",
    "    \n",
    "#     Args\n",
    "    \n",
    "#     \"\"\"\n",
    "        \n",
    "#     pipeline = Pipeline([\n",
    "#     ('features', FeatureUnion([\n",
    "#         ('text_pipeline', Pipeline([\n",
    "#             ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "#             ('tfidf', TfidfTransformer())\n",
    "#         ])),\n",
    "#         ('starting_verb', StartingVerbExtractor())\n",
    "#     ])),\n",
    "#     ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "#     ])\n",
    "            \n",
    "#     # specify parameters for grid search\n",
    "#     parameters = {\n",
    "#         'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#         'clf__estimator__n_estimators': [10, 25, 50],\n",
    "#         'clf__estimator__min_samples_split': [2, 3, 4]\n",
    "#     }\n",
    "    \n",
    "#     # create grid search object\n",
    "#     # cv = GridSearchCV(estimator=pipeline, param_grid=parameters, verbose=2, cv=3, error_score='raise' )\n",
    "    \n",
    "#     return pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
